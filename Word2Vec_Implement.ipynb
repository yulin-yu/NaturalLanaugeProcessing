{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,re,csv\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.corpus import stopwords\n",
    "from numba import jit\n",
    "from numpy import argmax\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.................................................................................\n",
    "#... global variables\n",
    "#.................................................................................\n",
    "\n",
    "\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "randcounter = 10\n",
    "np_randcounter = 10\n",
    "\n",
    "\n",
    "vocab_size = 0\n",
    "hidden_size = 100\n",
    "uniqueWords = [\"\"]                      #... list of all unique tokens\n",
    "wordcodes = {}                          #... dictionary mapping of words to indices in uniqueWords\n",
    "wordcounts = Counter()                  #... how many times each token occurs\n",
    "samplingTable = []                      #... table to draw negative samples from\n",
    "origcounts = {}\n",
    "listneed= []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    global uniqueWords, wordcodes, wordcounts, origcounts\n",
    "    override = False\n",
    "    if override:\n",
    "        fullrec = pickle.load(open(\"w2v_fullrec.p\",\"rb\"))\n",
    "        wordcodes = pickle.load( open(\"w2v_wordcodes.p\",\"rb\"))\n",
    "        uniqueWords= pickle.load(open(\"w2v_uniqueWords.p\",\"rb\"))\n",
    "        wordcounts = pickle.load(open(\"w2v_wordcounts.p\",\"rb\"))\n",
    "        return fullrec\n",
    "\n",
    "\n",
    "   \n",
    "    handle = open(filename, \"r\", encoding=\"utf8\")\n",
    "    fullconts =handle.read().split(\"\\n\")\n",
    "    #fullconts = [entry.split(\"\\t\")[0].replace(\"<br />\", \"\") for entry in fullconts[1:150] ]\n",
    "    fullconts = [\" \".join(fullconts).lower()]\n",
    "    fullconts = re.sub('[^A-Za-z]+', ' ', fullconts[0])\n",
    "    fullconts = [fullconts]\n",
    "    \n",
    "   # print(fullconts)\n",
    "    print (\"Generating token stream...\")\n",
    "    stopword = set(stopwords.words('english'))\n",
    "    rawword = nltk.word_tokenize(fullconts[0])\n",
    "    fullrec = [x for x in rawword if not x in stopword]\n",
    "    min_count = 50\n",
    "    origcounts = Counter(fullrec)\n",
    "    \n",
    "    \n",
    "    print (\"Performing minimum thresholding..\")\n",
    "    fullrec_filtered = []\n",
    "    test1 = pd.read_csv('intrinsic-test.tsv' , sep='\\t')  \n",
    "    listneed = list(set(list(test1['word1']) + list(test1['word2'])))\n",
    "    for i in origcounts.keys():\n",
    "        if origcounts[i] >= min_count or i in listneed:\n",
    "            fullrec_filtered.append(i)   \n",
    "        else:\n",
    "            fullrec_filtered.append('<UNK>')\n",
    "     \n",
    "    #wordcounts[token] = origcounts[token]\n",
    "    wordcounts = Counter(fullrec_filtered)\n",
    "\n",
    "    print (\"Producing one-hot indicies\")\n",
    "    char_int = {v:k for k, v in enumerate(wordcounts.keys())}\n",
    "    #char_int = dict((k, v) for k, v in enumerate(wordcounts.keys()))\n",
    "    wordcodes = char_int\n",
    "    uniqueWords = list(Counter(fullrec_filtered).keys())\n",
    "\n",
    "\n",
    "    #print(uniqueWords)\n",
    "\n",
    "    uniqueword_Encoded = [char_int[char] for char in fullrec_filtered]\n",
    "    #onehotlist = []\n",
    "    #for i in uniqueword_Encoded:\n",
    "    #    number =  [0 for x in range(len(rawword))]\n",
    "    #    number[value] = 1\n",
    "    #    onehotlist.append(number)  \n",
    "        \n",
    "    #wordcodes = dict(zip(uniqueWords, onehotlist))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #... close input file handle\n",
    "    handle.close()\n",
    "\n",
    "\n",
    "    pickle.dump(fullrec, open(\"w2v_fullrec.p\",\"wb+\"))\n",
    "    pickle.dump(wordcodes, open(\"w2v_wordcodes.p\",\"wb+\"))\n",
    "    pickle.dump(uniqueWords, open(\"w2v_uniqueWords.p\",\"wb+\"))\n",
    "    pickle.dump(dict(wordcounts), open(\"w2v_wordcounts.p\",\"wb+\"))\n",
    "\n",
    "\n",
    "    return uniqueword_Encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negativeSampleTable(train_data, uniqueWords, wordcounts, exp_power=0.75):\n",
    "    global wordcodes\n",
    "    #... stores the normalizing denominator (count of all tokens, each count raised to exp_power)\n",
    "   # max_exp_count = 0\n",
    "    \n",
    "    print (\"Generating exponentiated count vectors\")\n",
    "\n",
    "\n",
    "     \n",
    "    exp_count_array = [v**exp_power for k,v in wordcounts.items()]\n",
    "    #print(exp_count_array)\n",
    "    #np.asarray(list(uniquedict1.values())) \n",
    "    max_exp_count = sum(exp_count_array)    \n",
    "    \n",
    "    print (\"Generating distribution\")\n",
    "    \n",
    "    prob_dist = []\n",
    "    for v in exp_count_array:\n",
    "        nor = v/max_exp_count\n",
    "        prob_dist.append(nor)\n",
    "        \n",
    "    print (\"Filling up sampling table\")\n",
    "    cumulative_dict = {}\n",
    "    key_c = 0\n",
    "    lendict = len(prob_dist)\n",
    "    table_size = 1e7\n",
    "    start = 0\n",
    "    for a in range(0,lendict):\n",
    "        for i in range(int(start), int(start + int(table_size*prob_dist[a]))):\n",
    "            cumulative_dict[i]=a\n",
    "        start+= table_size*prob_dist[a]\n",
    "    \n",
    "    return cumulative_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples(context_idx, num_samples):\n",
    "    global samplingTable, uniqueWords, randcounter\n",
    "    results = []\n",
    "    if len(results) < num_samples:\n",
    "        key1=np.random.randint(0,len(samplingTable))\n",
    "        try:\n",
    "            content1 = samplingTable[key1]\n",
    "            if content1 != context_idx:\n",
    "                results.append(content1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "\t#... (TASK) randomly sample num_samples token indices from samplingTable.\n",
    "\t#... don't allow the chosen token to be context_idx.\n",
    "\t#... append the chosen indices to results\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "def performDescent(num_samples, learning_rate, center_token,context_index,W1,W2,negative_indices):\n",
    "    # sequence chars was generated from the mapped sequence in the core code    \n",
    "    W1token = W1[center_token,:]\n",
    "    W2arrayind = W2[context_index,:]\n",
    "    positive = sigmoid(np.dot(W2arrayind,W1token.T))-1                   \n",
    "    psum = positive * W2arrayind\n",
    "    perror = learning_rate*positive*W1token\n",
    "    pgraident = W2arrayind - perror\n",
    "    n_sum = 0\n",
    "    nll = 0\n",
    "    for c in negative_indices:\n",
    "        W2arrary_c = W2[c,:]\n",
    "        nagative = np.array(sigmoid(-np.dot(W2arrary_c,W1token.T)))\n",
    "        n_sum = np.log(nagative) + n_sum\n",
    "        #a,b = W2arrary_c.shape\n",
    "        #column = int(b) - 1\n",
    "        #co = random.randint(0,column)\n",
    "        psum = psum + nagative*W2arrary_c\n",
    "        nerror = learning_rate*nagative*W1token\n",
    "        ngraident = W2arrary_c - nerror\n",
    "    serror = learning_rate * psum\n",
    "    W1token = W1token - serror\n",
    "    nll = -np.log(positive + 1) - n_sum\n",
    "    return nll\n",
    "                       \n",
    "\t\t#... (TASK) implement gradient descent. Find the current context token from sequence_chars\n",
    "\t\t#... and the associated negative samples from negative_indices. Run gradient descent on both\n",
    "\t\t#... weight matrices W1 and W2.\n",
    "\t\t#... compute the total negative log-likelihood and store this in nll_new.                       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(curW1 = None, curW2=None):\n",
    "    global uniqueWords, wordcodes, fullsequence, vocab_size, hidden_size,np_randcounter, randcounter\n",
    "    vocab_size = len(uniqueWords)           #... unique characters\n",
    "    hidden_size = 100                       #... number of hidden neurons\n",
    "    context_window = [-2,-1,1,2]            #... specifies which context indices are output. Indices relative to target word. Don't include index 0 itself.\n",
    "    nll_results = []                        #... keep array of negative log-likelihood after every 1000 iterations\n",
    "    context_index = []\n",
    "\n",
    "    #... determine how much of the full sequence we can use while still accommodating the context window\n",
    "    start_point = int(math.fabs(min(context_window)))\n",
    "    end_point = len(fullsequence)-(max(max(context_window),0))\n",
    "    mapped_sequence = fullsequence\n",
    "\n",
    "\n",
    "\n",
    "\t#... initialize the weight matrices. W1 is from input->hidden and W2 is from hidden->output.\n",
    "    if curW1==None:\n",
    "        np_randcounter += 1\n",
    "        W1 = np.random.uniform(-.5, .5, size=(vocab_size, hidden_size))\n",
    "        W2 = np.random.uniform(-.5, .5, size=(vocab_size, hidden_size))\n",
    "    else:\n",
    "\t\t#... initialized from pre-loaded file\n",
    "        W1 = curW1\n",
    "        W2 = curW2\n",
    "\n",
    "\n",
    "\n",
    "\t#... set the training parameters\n",
    "    epochs = 10\n",
    "    num_samples = 2\n",
    "    learning_rate = 0.05\n",
    "    nll = 0\n",
    "    iternum = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t#... Begin actual training\n",
    "    for j in range(0,epochs):\n",
    "        print (\"Epoch: \", j)\n",
    "        prevmark = 0\n",
    "\n",
    "\t\t#... For each epoch, redo the whole sequence...\n",
    "        for i in range(start_point,end_point):\n",
    "\n",
    "            if (float(i)/len(mapped_sequence))>=(prevmark+0.1):\n",
    "                print (\"Progress: \", round(prevmark+0.1,1))\n",
    "                prevmark += 0.1\n",
    "            if iternum%10000==0:\n",
    "                print (\"Negative likelihood: \", nll)\n",
    "                nll_results.append(nll)\n",
    "                nll = 0\n",
    "\n",
    "\n",
    "\t\t\t#... (TASK) determine which token is our current input. Remember that we're looping through mapped_sequence\n",
    "#            center_token = mapped_sequence[i]\n",
    "\n",
    "            if mapped_sequence[i] != '<UNK>':\n",
    "                center_token = mapped_sequence[i]\n",
    "        \n",
    "                #... fill in\n",
    "\t\t\t#... (TASK) don't allow the center_token to be <UNK>. move to next iteration if you found <UNK>.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            iternum += 1\n",
    "\t\t\t#... now propagate to each of the context outputs\n",
    "            for k in range(0, len(context_window)):\n",
    "\n",
    "\t\t\t\t#... (TASK) Use context_window to find one-hot index of the current context token.\n",
    "                context_index = fullsequence[context_window[k]+i]\n",
    "                if context_index == wordcodes['<UNK>']:\n",
    "                    continue\n",
    "                #context_index = #... fill in\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t\t#... construct some negative samples\n",
    "                #negative_indices = []\n",
    "                \n",
    "                negative_indices = generateSamples(context_index, num_samples)\n",
    "\t\t\t\t#... (TASK) You have your context token and your negative samples.\n",
    "\t\t\t\t#... Perform gradient descent on both weight matrices.\n",
    "\t\t\t\n",
    "            #... Also keep track of the negative log-likelihood in variable nll.\n",
    "                nll = performDescent(num_samples, learning_rate, center_token,context_index,W1,W2,negative_indices)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for nll_res in nll_results:\n",
    "        print (nll_res)\n",
    "    return [W1,W2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "\thandle = open(\"saved_W1.data\",\"rb\")\n",
    "\tW1 = np.load(handle)\n",
    "\thandle.close()\n",
    "\thandle = open(\"saved_W2.data\",\"rb\")\n",
    "\tW2 = np.load(handle)\n",
    "\thandle.close()\n",
    "\treturn [W1,W2]\n",
    "\n",
    "\n",
    "def save_model(W1,W2):\n",
    "\thandle = open(\"saved_W1.data\",\"wb+\")\n",
    "\tnp.save(handle, W1, allow_pickle=False)\n",
    "\thandle.close()\n",
    "\n",
    "\thandle = open(\"saved_W2.data\",\"wb+\")\n",
    "\tnp.save(handle, W2, allow_pickle=False)\n",
    "\thandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = []\n",
    "proj_embeddings = []\n",
    "def train_vectors(preload=False):\n",
    "\tglobal word_embeddings, proj_embeddings\n",
    "\tif preload:\n",
    "\t\t[curW1, curW2] = load_model()\n",
    "\telse:\n",
    "\t\tcurW1 = None\n",
    "\t\tcurW2 = None\n",
    "\t[word_embeddings, proj_embeddings] = trainer(curW1,curW2)\n",
    "\tsave_model(word_embeddings, proj_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphology(word_seq):\n",
    "\tglobal word_embeddings, proj_embeddings, uniqueWords, wordcodes\n",
    "\tembeddings = word_embeddings\n",
    "\tvectors = [word_seq[0], # suffix averaged\n",
    "\tembeddings[wordcodes[word_seq[1]]]]\n",
    "\tvector_math = vectors[0]+vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word_seq):\n",
    "\tglobal word_embeddings, proj_embeddings, uniqueWords, wordcodes\n",
    "\tembeddings = word_embeddings\n",
    "\tvectors = [embeddings[wordcodes[word_seq[0]]],\n",
    "\tembeddings[wordcodes[word_seq[1]]],\n",
    "\tembeddings[wordcodes[word_seq[2]]]]\n",
    "\tvector_math = -vectors[0] + vectors[1] - vectors[2] # + vectors[3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_neighbors(target_word):\n",
    "\tglobal word_embeddings, proj_embeddings, uniqueWords, wordcodes\n",
    "\ttargets = [target_word]\n",
    "\toutputs = []\n",
    "\toutput_d = {}\n",
    "\tind_t = wordcodes[target_word]   \n",
    "\tfor i in uniqueWords:\n",
    "\t\tind_s = wordcodes[i]\n",
    "\t\tv_s = proj_embeddings[ind_s]\n",
    "\t\tv_t = proj_embeddings[ind_t]\n",
    "\t\tscore1 = 1-cosine(v_s,v_t)\n",
    "\t\toutput_d[i] = score1\n",
    "\tkey = sorted(output_d, key=output_d.get, reverse=True)[:10]\n",
    "    #value = sorted(output_d.values(),reverse=True)[:10]\n",
    "\tfor a in key:   \n",
    "\t\telem = {'word':a, \"score\": float(output_d[a])}\n",
    "\t\toutputs.append(elem)\n",
    "\treturn outputs\n",
    "        \n",
    "#\t\t\t\t\tind1 = wordcodes[word1]\n",
    "#\t\t\t\t\tvectornumber1 = word_embeddings[ind1]\n",
    "#\t\t\t\t\tind2 = wordcodes[word2]\n",
    "#\t\t\t\t\tvectornumber2 = word_embeddings[ind2]\n",
    "#\t\t\t\t\tScore = abs(1-cosine(vectornumber1,vectornumber2))\n",
    "\t#... (TASK) search through all uniqueWords and for each token, compute its similarity to target_word.\n",
    "\t#... you will compute this using the absolute cosine similarity of the word_embeddings for the word pairs.\n",
    "\t#... Note that the cosine() function from scipy.spatial.distance computes a DISTANCE so you need to convert that to a similarity.\n",
    "\t#... return a list of top 10 most similar words in the form of dicts,\n",
    "\t#... each dict having format: {\"word\":<token_name>, \"score\":<cosine_similarity>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating token stream...\n",
      "Performing minimum thresholding..\n",
      "Producing one-hot indicies\n",
      "Full sequence loaded...\n",
      "Total unique words:  13831\n",
      "Preparing negative sampling table\n",
      "Generating exponentiated count vectors\n",
      "Generating distribution\n",
      "Filling up sampling table\n",
      "Epoch:  0\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  2.229018561387811\n",
      "Negative likelihood:  1.2082835654293254\n",
      "Negative likelihood:  1.7158322905872416\n",
      "Progress:  0.1\n",
      "Negative likelihood:  1.5220413731059004\n",
      "Negative likelihood:  2.7258790227007568\n",
      "Negative likelihood:  1.5014420322920685\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.265258489314324\n",
      "Negative likelihood:  1.903656854006797\n",
      "Negative likelihood:  1.309769260912562\n",
      "Progress:  0.3\n",
      "Negative likelihood:  1.6368614287771444\n",
      "Negative likelihood:  1.3614637800104168\n",
      "Negative likelihood:  2.276732193888079\n",
      "Progress:  0.4\n",
      "Negative likelihood:  1.3820849080079256\n",
      "Negative likelihood:  1.3980353901252935\n",
      "Negative likelihood:  1.309512348870047\n",
      "Negative likelihood:  1.3082198285443065\n",
      "Progress:  0.5\n",
      "Negative likelihood:  1.1110569964076167\n",
      "Negative likelihood:  2.747348648357631\n",
      "Negative likelihood:  0\n",
      "Progress:  0.6\n",
      "Negative likelihood:  2.1007086059125166\n",
      "Negative likelihood:  1.1071322409906832\n",
      "Negative likelihood:  1.3544216978557326\n",
      "Progress:  0.7\n",
      "Negative likelihood:  1.498320214435517\n",
      "Negative likelihood:  1.218295044259796\n",
      "Negative likelihood:  1.732716990425975\n",
      "Progress:  0.8\n",
      "Negative likelihood:  1.7987365707057155\n",
      "Negative likelihood:  2.138998626037182\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  0.8363887009181123\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.1458154519180441\n",
      "Negative likelihood:  1.9842978447722928\n",
      "Negative likelihood:  1.540416549426566\n",
      "Epoch:  1\n",
      "Negative likelihood:  1.4672403896839508\n",
      "Negative likelihood:  1.1852565222241016\n",
      "Negative likelihood:  1.428777313649026\n",
      "Progress:  0.1\n",
      "Negative likelihood:  2.3190343498516723\n",
      "Negative likelihood:  1.187541417081841\n",
      "Negative likelihood:  1.8275213095565932\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.9382500370218345\n",
      "Negative likelihood:  1.1779330659604312\n",
      "Negative likelihood:  2.7870910519854677\n",
      "Progress:  0.3\n",
      "Negative likelihood:  0.9845067295695867\n",
      "Negative likelihood:  2.54244948379175\n",
      "Negative likelihood:  1.7679279415133782\n",
      "Negative likelihood:  1.082892830869069\n",
      "Progress:  0.4\n",
      "Negative likelihood:  2.283587434253856\n",
      "Negative likelihood:  0.7851715420685441\n",
      "Negative likelihood:  1.0087005473118071\n",
      "Progress:  0.5\n",
      "Negative likelihood:  1.0444561508003618\n",
      "Negative likelihood:  2.554184854730233\n",
      "Negative likelihood:  0\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.9774815214412187\n",
      "Negative likelihood:  1.26181888493696\n",
      "Negative likelihood:  1.4515270815881856\n",
      "Progress:  0.7\n",
      "Negative likelihood:  2.049102559128857\n",
      "Negative likelihood:  1.9921914273691783\n",
      "Negative likelihood:  2.1073994637144207\n",
      "Negative likelihood:  1.6241891875468193\n",
      "Progress:  0.8\n",
      "Negative likelihood:  1.916896495293439\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  1.59931318284043\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.1458154519180441\n",
      "Negative likelihood:  1.9842978447722928\n",
      "Negative likelihood:  1.470140184675537\n",
      "Epoch:  2\n",
      "Negative likelihood:  1.8733116420407963\n",
      "Negative likelihood:  1.2623312758027496\n",
      "Negative likelihood:  0.9923742524161758\n",
      "Progress:  0.1\n",
      "Negative likelihood:  1.5817036885711364\n",
      "Negative likelihood:  1.906895023211605\n",
      "Negative likelihood:  1.5234622240941937\n",
      "Negative likelihood:  0.9772069172842383\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.6898665952398177\n",
      "Negative likelihood:  2.943323232810849\n",
      "Negative likelihood:  1.5737128725196192\n",
      "Progress:  0.3\n",
      "Negative likelihood:  1.6517680865663569\n",
      "Negative likelihood:  2.3472430695216673\n",
      "Negative likelihood:  1.8793186738202552\n",
      "Progress:  0.4\n",
      "Negative likelihood:  1.9173757147278225\n",
      "Negative likelihood:  1.007979046590711\n",
      "Negative likelihood:  3.509583820142906\n",
      "Progress:  0.5\n",
      "Negative likelihood:  1.8588125639848658\n",
      "Negative likelihood:  1.0551126950809098\n",
      "Negative likelihood:  3.09901066055418\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.618200673180822\n",
      "Negative likelihood:  1.4451627362489905\n",
      "Negative likelihood:  1.4946597419130607\n",
      "Negative likelihood:  1.3936020919899752\n",
      "Progress:  0.7\n",
      "Negative likelihood:  2.072047438019028\n",
      "Negative likelihood:  1.748523567690706\n",
      "Negative likelihood:  1.6933140756485952\n",
      "Progress:  0.8\n",
      "Negative likelihood:  1.916896495293439\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  0\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.8984308528020317\n",
      "Negative likelihood:  2.0525854472469707\n",
      "Negative likelihood:  2.2705183244500122\n",
      "Epoch:  3\n",
      "Negative likelihood:  1.0152095218471704\n",
      "Negative likelihood:  1.4185229506716563\n",
      "Negative likelihood:  0.9874575539145675\n",
      "Negative likelihood:  1.62616262057259\n",
      "Progress:  0.1\n",
      "Negative likelihood:  1.5812966941577884\n",
      "Negative likelihood:  1.6276910942411433\n",
      "Negative likelihood:  0.9443133945198522\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.8633155215453936\n",
      "Negative likelihood:  1.5175413145403636\n",
      "Negative likelihood:  1.4657585569474025\n",
      "Progress:  0.3\n",
      "Negative likelihood:  1.2794762624672003\n",
      "Negative likelihood:  2.197291059463995\n",
      "Negative likelihood:  2.276732193888079\n",
      "Progress:  0.4\n",
      "Negative likelihood:  1.3820849080079256\n",
      "Negative likelihood:  1.2619582872827004\n",
      "Negative likelihood:  1.2542381839471415\n",
      "Progress:  0.5\n",
      "Negative likelihood:  2.0457527710667316\n",
      "Negative likelihood:  2.0533097511697456\n",
      "Negative likelihood:  2.747348648357631\n",
      "Negative likelihood:  1.1916975644318426\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.138051867084579\n",
      "Negative likelihood:  1.0395978312454295\n",
      "Negative likelihood:  1.3936020919899752\n",
      "Progress:  0.7\n",
      "Negative likelihood:  2.183211776364269\n",
      "Negative likelihood:  1.0010992704173747\n",
      "Negative likelihood:  1.6241891875468193\n",
      "Progress:  0.8\n",
      "Negative likelihood:  1.916896495293439\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  0\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.4533733043915624\n",
      "Negative likelihood:  0.6490439258169942\n",
      "Negative likelihood:  0.8913110784906386\n",
      "Negative likelihood:  1.6242486750078147\n",
      "Epoch:  4\n",
      "Negative likelihood:  1.2401503007466288\n",
      "Negative likelihood:  1.395646860432478\n",
      "Negative likelihood:  1.8168109661913499\n",
      "Progress:  0.1\n",
      "Negative likelihood:  1.1975701981518567\n",
      "Negative likelihood:  1.7526273338028404\n",
      "Negative likelihood:  1.2037454115575008\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.0831500569200974\n",
      "Negative likelihood:  2.2210714235807094\n",
      "Negative likelihood:  1.610824427546378\n",
      "Progress:  0.3\n",
      "Negative likelihood:  2.409138371920377\n",
      "Negative likelihood:  1.649239286474499\n",
      "Negative likelihood:  2.0714776873883833\n",
      "Negative likelihood:  0.7246681646799662\n",
      "Progress:  0.4\n",
      "Negative likelihood:  1.157362628664219\n",
      "Negative likelihood:  1.6902752742676883\n",
      "Negative likelihood:  1.422885192308656\n",
      "Progress:  0.5\n",
      "Negative likelihood:  1.4212582038494452\n",
      "Negative likelihood:  2.8296896803178444\n",
      "Negative likelihood:  0\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.2401231066342882\n",
      "Negative likelihood:  1.5881124300568075\n",
      "Negative likelihood:  2.0091964444422663\n",
      "Progress:  0.7\n",
      "Negative likelihood:  1.9331914126370977\n",
      "Negative likelihood:  1.9921914273691783\n",
      "Negative likelihood:  1.4674209930080955\n",
      "Progress:  0.8\n",
      "Negative likelihood:  1.6241891875468193\n",
      "Negative likelihood:  1.916896495293439\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  1.8472778208263725\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.1458154519180441\n",
      "Negative likelihood:  1.9842978447722928\n",
      "Negative likelihood:  0.8885353200300123\n",
      "Epoch:  5\n",
      "Negative likelihood:  1.4869204969123395\n",
      "Negative likelihood:  2.1496484346429403\n",
      "Negative likelihood:  1.0813050599465481\n",
      "Progress:  0.1\n",
      "Negative likelihood:  2.5451220602744495\n",
      "Negative likelihood:  1.2530001809680493\n",
      "Negative likelihood:  0.6665057112678736\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.3434804583486675\n",
      "Negative likelihood:  1.4285145401593287\n",
      "Negative likelihood:  1.3868754921106548\n",
      "Negative likelihood:  2.2496360173345664\n",
      "Progress:  0.3\n",
      "Negative likelihood:  1.1933730904053965\n",
      "Negative likelihood:  3.6988558724298013\n",
      "Negative likelihood:  2.334482312091863\n",
      "Progress:  0.4\n",
      "Negative likelihood:  1.3041274948478643\n",
      "Negative likelihood:  1.5168969720007106\n",
      "Negative likelihood:  1.4376615427344661\n",
      "Progress:  0.5\n",
      "Negative likelihood:  1.6505248490356048\n",
      "Negative likelihood:  2.747348648357631\n",
      "Negative likelihood:  0\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.6880771871049873\n",
      "Negative likelihood:  1.1644583388362726\n",
      "Negative likelihood:  1.084974540800041\n",
      "Negative likelihood:  1.2606735249950174\n",
      "Progress:  0.7\n",
      "Negative likelihood:  1.404592743512676\n",
      "Negative likelihood:  2.1073994637144207\n",
      "Negative likelihood:  0.9971382466756593\n",
      "Progress:  0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative likelihood:  1.916896495293439\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  1.7639237723730876\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.0147189157706795\n",
      "Negative likelihood:  0.8498462420420243\n",
      "Negative likelihood:  1.470140184675537\n",
      "Epoch:  6\n",
      "Negative likelihood:  0.9273838696195298\n",
      "Negative likelihood:  1.520906915164474\n",
      "Negative likelihood:  1.2733160631995084\n",
      "Progress:  0.1\n",
      "Negative likelihood:  1.2930372955584768\n",
      "Negative likelihood:  2.188385371848345\n",
      "Negative likelihood:  1.9418589024051576\n",
      "Negative likelihood:  2.2504843503233882\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.0856825805704169\n",
      "Negative likelihood:  1.839279776153659\n",
      "Negative likelihood:  1.0978629620741496\n",
      "Progress:  0.3\n",
      "Negative likelihood:  1.4735296273573564\n",
      "Negative likelihood:  2.475328945912882\n",
      "Negative likelihood:  1.7475884698266526\n",
      "Progress:  0.4\n",
      "Negative likelihood:  1.8239783050220528\n",
      "Negative likelihood:  1.6267350663054283\n",
      "Negative likelihood:  1.309512348870047\n",
      "Progress:  0.5\n",
      "Negative likelihood:  1.8190213566755518\n",
      "Negative likelihood:  1.9757039747792617\n",
      "Negative likelihood:  2.5531730037503957\n",
      "Negative likelihood:  0.6631180323586892\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.8754953045769098\n",
      "Negative likelihood:  1.4946597419130607\n",
      "Negative likelihood:  0.9674713303683659\n",
      "Progress:  0.7\n",
      "Negative likelihood:  2.072047438019028\n",
      "Negative likelihood:  1.320247928471328\n",
      "Negative likelihood:  2.7236231404602984\n",
      "Progress:  0.8\n",
      "Negative likelihood:  1.916896495293439\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  0\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.3699820084242427\n",
      "Negative likelihood:  1.5920216300739536\n",
      "Negative likelihood:  1.9177872852113271\n",
      "Epoch:  7\n",
      "Negative likelihood:  1.4762703461269207\n",
      "Negative likelihood:  1.8517786494513309\n",
      "Negative likelihood:  2.038184060942188\n",
      "Negative likelihood:  1.342891800436652\n",
      "Progress:  0.1\n",
      "Negative likelihood:  1.3159204035754435\n",
      "Negative likelihood:  2.263886907710258\n",
      "Negative likelihood:  1.4714967618799704\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.1337207594624328\n",
      "Negative likelihood:  1.5382524484903382\n",
      "Negative likelihood:  1.5259144024950269\n",
      "Progress:  0.3\n",
      "Negative likelihood:  1.6355156850930581\n",
      "Negative likelihood:  1.3614637800104168\n",
      "Negative likelihood:  2.9197393303179746\n",
      "Progress:  0.4\n",
      "Negative likelihood:  1.053328845500495\n",
      "Negative likelihood:  1.2194854160953046\n",
      "Negative likelihood:  0.844016177964298\n",
      "Negative likelihood:  1.9715081142623545\n",
      "Progress:  0.5\n",
      "Negative likelihood:  0.7576507817427693\n",
      "Negative likelihood:  2.3036070106672337\n",
      "Negative likelihood:  0\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.4037075841167415\n",
      "Negative likelihood:  1.9824838586667441\n",
      "Negative likelihood:  1.9130827550571854\n",
      "Progress:  0.7\n",
      "Negative likelihood:  1.8645725718462114\n",
      "Negative likelihood:  1.608515169655198\n",
      "Negative likelihood:  1.6241891875468193\n",
      "Progress:  0.8\n",
      "Negative likelihood:  1.4549848772744132\n",
      "Negative likelihood:  1.916896495293439\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  1.4634396417892725\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.3558553582346369\n",
      "Negative likelihood:  1.9842978447722928\n",
      "Negative likelihood:  1.71396655722757\n",
      "Epoch:  8\n",
      "Negative likelihood:  1.396162383622908\n",
      "Negative likelihood:  1.4096033574098792\n",
      "Negative likelihood:  1.160454098309102\n",
      "Progress:  0.1\n",
      "Negative likelihood:  1.8669302983401566\n",
      "Negative likelihood:  0.9975944272707127\n",
      "Negative likelihood:  1.8275213095565932\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.9382500370218345\n",
      "Negative likelihood:  1.1779330659604312\n",
      "Negative likelihood:  1.6151532215618927\n",
      "Progress:  0.3\n",
      "Negative likelihood:  1.008230030978943\n",
      "Negative likelihood:  2.977611554052295\n",
      "Negative likelihood:  1.7679279415133782\n",
      "Negative likelihood:  1.082892830869069\n",
      "Progress:  0.4\n",
      "Negative likelihood:  2.0794294415763273\n",
      "Negative likelihood:  1.309512348870047\n",
      "Negative likelihood:  0.5745433466418883\n",
      "Progress:  0.5\n",
      "Negative likelihood:  0.7851957899792448\n",
      "Negative likelihood:  2.5066654387959284\n",
      "Negative likelihood:  0\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.468696456558841\n",
      "Negative likelihood:  1.4759767627073577\n",
      "Negative likelihood:  1.4946597419130607\n",
      "Progress:  0.7\n",
      "Negative likelihood:  2.80560469421928\n",
      "Negative likelihood:  1.9921914273691783\n",
      "Negative likelihood:  2.1073994637144207\n",
      "Negative likelihood:  1.6241891875468193\n",
      "Progress:  0.8\n",
      "Negative likelihood:  1.913501639213056\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  1.8927701431593371\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.1458154519180441\n",
      "Negative likelihood:  2.782497612258097\n",
      "Negative likelihood:  1.470140184675537\n",
      "Epoch:  9\n",
      "Negative likelihood:  1.1990714981044035\n",
      "Negative likelihood:  2.006617281154012\n",
      "Negative likelihood:  1.987584603982091\n",
      "Progress:  0.1\n",
      "Negative likelihood:  1.4843688889605562\n",
      "Negative likelihood:  1.906895023211605\n",
      "Negative likelihood:  1.6419584171125248\n",
      "Negative likelihood:  1.4033376789058476\n",
      "Progress:  0.2\n",
      "Negative likelihood:  1.3778892494554484\n",
      "Negative likelihood:  2.3939349428362653\n",
      "Negative likelihood:  1.5737128725196192\n",
      "Progress:  0.3\n",
      "Negative likelihood:  1.1492418013610393\n",
      "Negative likelihood:  1.2332941303390736\n",
      "Negative likelihood:  1.8793186738202552\n",
      "Progress:  0.4\n",
      "Negative likelihood:  1.8239783050220528\n",
      "Negative likelihood:  1.007979046590711\n",
      "Negative likelihood:  2.773071870411727\n",
      "Progress:  0.5\n",
      "Negative likelihood:  1.8588125639848658\n",
      "Negative likelihood:  1.0551126950809098\n",
      "Negative likelihood:  2.2777906048361527\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.618200673180822\n",
      "Negative likelihood:  1.8858994584389668\n",
      "Negative likelihood:  1.4946597419130607\n",
      "Negative likelihood:  1.3936020919899752\n",
      "Progress:  0.7\n",
      "Negative likelihood:  3.2525801208693794\n",
      "Negative likelihood:  1.7353825733582902\n",
      "Negative likelihood:  1.6241891875468193\n",
      "Progress:  0.8\n",
      "Negative likelihood:  1.7364047471970867\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  0\n",
      "Progress:  0.9\n",
      "Negative likelihood:  1.0147189157706795\n",
      "Negative likelihood:  2.0525854472469707\n",
      "Negative likelihood:  3.0125370890301912\n",
      "0\n",
      "2.229018561387811\n",
      "1.2082835654293254\n",
      "1.7158322905872416\n",
      "1.5220413731059004\n",
      "2.7258790227007568\n",
      "1.5014420322920685\n",
      "1.265258489314324\n",
      "1.903656854006797\n",
      "1.309769260912562\n",
      "1.6368614287771444\n",
      "1.3614637800104168\n",
      "2.276732193888079\n",
      "1.3820849080079256\n",
      "1.3980353901252935\n",
      "1.309512348870047\n",
      "1.3082198285443065\n",
      "1.1110569964076167\n",
      "2.747348648357631\n",
      "0\n",
      "2.1007086059125166\n",
      "1.1071322409906832\n",
      "1.3544216978557326\n",
      "1.498320214435517\n",
      "1.218295044259796\n",
      "1.732716990425975\n",
      "1.7987365707057155\n",
      "2.138998626037182\n",
      "0\n",
      "0.8363887009181123\n",
      "1.1458154519180441\n",
      "1.9842978447722928\n",
      "1.540416549426566\n",
      "1.4672403896839508\n",
      "1.1852565222241016\n",
      "1.428777313649026\n",
      "2.3190343498516723\n",
      "1.187541417081841\n",
      "1.8275213095565932\n",
      "1.9382500370218345\n",
      "1.1779330659604312\n",
      "2.7870910519854677\n",
      "0.9845067295695867\n",
      "2.54244948379175\n",
      "1.7679279415133782\n",
      "1.082892830869069\n",
      "2.283587434253856\n",
      "0.7851715420685441\n",
      "1.0087005473118071\n",
      "1.0444561508003618\n",
      "2.554184854730233\n",
      "0\n",
      "1.9774815214412187\n",
      "1.26181888493696\n",
      "1.4515270815881856\n",
      "2.049102559128857\n",
      "1.9921914273691783\n",
      "2.1073994637144207\n",
      "1.6241891875468193\n",
      "1.916896495293439\n",
      "0\n",
      "1.59931318284043\n",
      "1.1458154519180441\n",
      "1.9842978447722928\n",
      "1.470140184675537\n",
      "1.8733116420407963\n",
      "1.2623312758027496\n",
      "0.9923742524161758\n",
      "1.5817036885711364\n",
      "1.906895023211605\n",
      "1.5234622240941937\n",
      "0.9772069172842383\n",
      "1.6898665952398177\n",
      "2.943323232810849\n",
      "1.5737128725196192\n",
      "1.6517680865663569\n",
      "2.3472430695216673\n",
      "1.8793186738202552\n",
      "1.9173757147278225\n",
      "1.007979046590711\n",
      "3.509583820142906\n",
      "1.8588125639848658\n",
      "1.0551126950809098\n",
      "3.09901066055418\n",
      "1.618200673180822\n",
      "1.4451627362489905\n",
      "1.4946597419130607\n",
      "1.3936020919899752\n",
      "2.072047438019028\n",
      "1.748523567690706\n",
      "1.6933140756485952\n",
      "1.916896495293439\n",
      "0\n",
      "0\n",
      "1.8984308528020317\n",
      "2.0525854472469707\n",
      "2.2705183244500122\n",
      "1.0152095218471704\n",
      "1.4185229506716563\n",
      "0.9874575539145675\n",
      "1.62616262057259\n",
      "1.5812966941577884\n",
      "1.6276910942411433\n",
      "0.9443133945198522\n",
      "1.8633155215453936\n",
      "1.5175413145403636\n",
      "1.4657585569474025\n",
      "1.2794762624672003\n",
      "2.197291059463995\n",
      "2.276732193888079\n",
      "1.3820849080079256\n",
      "1.2619582872827004\n",
      "1.2542381839471415\n",
      "2.0457527710667316\n",
      "2.0533097511697456\n",
      "2.747348648357631\n",
      "1.1916975644318426\n",
      "1.138051867084579\n",
      "1.0395978312454295\n",
      "1.3936020919899752\n",
      "2.183211776364269\n",
      "1.0010992704173747\n",
      "1.6241891875468193\n",
      "1.916896495293439\n",
      "0\n",
      "0\n",
      "1.4533733043915624\n",
      "0.6490439258169942\n",
      "0.8913110784906386\n",
      "1.6242486750078147\n",
      "1.2401503007466288\n",
      "1.395646860432478\n",
      "1.8168109661913499\n",
      "1.1975701981518567\n",
      "1.7526273338028404\n",
      "1.2037454115575008\n",
      "1.0831500569200974\n",
      "2.2210714235807094\n",
      "1.610824427546378\n",
      "2.409138371920377\n",
      "1.649239286474499\n",
      "2.0714776873883833\n",
      "0.7246681646799662\n",
      "1.157362628664219\n",
      "1.6902752742676883\n",
      "1.422885192308656\n",
      "1.4212582038494452\n",
      "2.8296896803178444\n",
      "0\n",
      "1.2401231066342882\n",
      "1.5881124300568075\n",
      "2.0091964444422663\n",
      "1.9331914126370977\n",
      "1.9921914273691783\n",
      "1.4674209930080955\n",
      "1.6241891875468193\n",
      "1.916896495293439\n",
      "0\n",
      "1.8472778208263725\n",
      "1.1458154519180441\n",
      "1.9842978447722928\n",
      "0.8885353200300123\n",
      "1.4869204969123395\n",
      "2.1496484346429403\n",
      "1.0813050599465481\n",
      "2.5451220602744495\n",
      "1.2530001809680493\n",
      "0.6665057112678736\n",
      "1.3434804583486675\n",
      "1.4285145401593287\n",
      "1.3868754921106548\n",
      "2.2496360173345664\n",
      "1.1933730904053965\n",
      "3.6988558724298013\n",
      "2.334482312091863\n",
      "1.3041274948478643\n",
      "1.5168969720007106\n",
      "1.4376615427344661\n",
      "1.6505248490356048\n",
      "2.747348648357631\n",
      "0\n",
      "1.6880771871049873\n",
      "1.1644583388362726\n",
      "1.084974540800041\n",
      "1.2606735249950174\n",
      "1.404592743512676\n",
      "2.1073994637144207\n",
      "0.9971382466756593\n",
      "1.916896495293439\n",
      "0\n",
      "1.7639237723730876\n",
      "1.0147189157706795\n",
      "0.8498462420420243\n",
      "1.470140184675537\n",
      "0.9273838696195298\n",
      "1.520906915164474\n",
      "1.2733160631995084\n",
      "1.2930372955584768\n",
      "2.188385371848345\n",
      "1.9418589024051576\n",
      "2.2504843503233882\n",
      "1.0856825805704169\n",
      "1.839279776153659\n",
      "1.0978629620741496\n",
      "1.4735296273573564\n",
      "2.475328945912882\n",
      "1.7475884698266526\n",
      "1.8239783050220528\n",
      "1.6267350663054283\n",
      "1.309512348870047\n",
      "1.8190213566755518\n",
      "1.9757039747792617\n",
      "2.5531730037503957\n",
      "0.6631180323586892\n",
      "1.8754953045769098\n",
      "1.4946597419130607\n",
      "0.9674713303683659\n",
      "2.072047438019028\n",
      "1.320247928471328\n",
      "2.7236231404602984\n",
      "1.916896495293439\n",
      "0\n",
      "0\n",
      "1.3699820084242427\n",
      "1.5920216300739536\n",
      "1.9177872852113271\n",
      "1.4762703461269207\n",
      "1.8517786494513309\n",
      "2.038184060942188\n",
      "1.342891800436652\n",
      "1.3159204035754435\n",
      "2.263886907710258\n",
      "1.4714967618799704\n",
      "1.1337207594624328\n",
      "1.5382524484903382\n",
      "1.5259144024950269\n",
      "1.6355156850930581\n",
      "1.3614637800104168\n",
      "2.9197393303179746\n",
      "1.053328845500495\n",
      "1.2194854160953046\n",
      "0.844016177964298\n",
      "1.9715081142623545\n",
      "0.7576507817427693\n",
      "2.3036070106672337\n",
      "0\n",
      "1.4037075841167415\n",
      "1.9824838586667441\n",
      "1.9130827550571854\n",
      "1.8645725718462114\n",
      "1.608515169655198\n",
      "1.6241891875468193\n",
      "1.4549848772744132\n",
      "1.916896495293439\n",
      "0\n",
      "1.4634396417892725\n",
      "1.3558553582346369\n",
      "1.9842978447722928\n",
      "1.71396655722757\n",
      "1.396162383622908\n",
      "1.4096033574098792\n",
      "1.160454098309102\n",
      "1.8669302983401566\n",
      "0.9975944272707127\n",
      "1.8275213095565932\n",
      "1.9382500370218345\n",
      "1.1779330659604312\n",
      "1.6151532215618927\n",
      "1.008230030978943\n",
      "2.977611554052295\n",
      "1.7679279415133782\n",
      "1.082892830869069\n",
      "2.0794294415763273\n",
      "1.309512348870047\n",
      "0.5745433466418883\n",
      "0.7851957899792448\n",
      "2.5066654387959284\n",
      "0\n",
      "1.468696456558841\n",
      "1.4759767627073577\n",
      "1.4946597419130607\n",
      "2.80560469421928\n",
      "1.9921914273691783\n",
      "2.1073994637144207\n",
      "1.6241891875468193\n",
      "1.913501639213056\n",
      "0\n",
      "1.8927701431593371\n",
      "1.1458154519180441\n",
      "2.782497612258097\n",
      "1.470140184675537\n",
      "1.1990714981044035\n",
      "2.006617281154012\n",
      "1.987584603982091\n",
      "1.4843688889605562\n",
      "1.906895023211605\n",
      "1.6419584171125248\n",
      "1.4033376789058476\n",
      "1.3778892494554484\n",
      "2.3939349428362653\n",
      "1.5737128725196192\n",
      "1.1492418013610393\n",
      "1.2332941303390736\n",
      "1.8793186738202552\n",
      "1.8239783050220528\n",
      "1.007979046590711\n",
      "2.773071870411727\n",
      "1.8588125639848658\n",
      "1.0551126950809098\n",
      "2.2777906048361527\n",
      "1.618200673180822\n",
      "1.8858994584389668\n",
      "1.4946597419130607\n",
      "1.3936020919899752\n",
      "3.2525801208693794\n",
      "1.7353825733582902\n",
      "1.6241891875468193\n",
      "1.7364047471970867\n",
      "0\n",
      "0\n",
      "1.0147189157706795\n",
      "2.0525854472469707\n",
      "3.0125370890301912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  coast\n",
      "{'word': 'coast', 'score': 1.0}\n",
      "{'word': 'extension', 'score': 0.36132932210196356}\n",
      "{'word': 'editors', 'score': 0.3513343756954652}\n",
      "{'word': 'actually', 'score': 0.3428329580559405}\n",
      "{'word': 'soils', 'score': 0.3371185593519669}\n",
      "{'word': 'ariel', 'score': 0.334087495369884}\n",
      "{'word': 'olivia', 'score': 0.3283927190974989}\n",
      "{'word': 'pseudonym', 'score': 0.32632901344802}\n",
      "{'word': 'ashamed', 'score': 0.3236031243645989}\n",
      "{'word': 'cop', 'score': 0.31757165754577854}\n",
      "\n",
      "\n",
      "Target:  london\n",
      "{'word': 'london', 'score': 1.0}\n",
      "{'word': 'eritrea', 'score': 0.33683285118741324}\n",
      "{'word': 'magnets', 'score': 0.3333886564095164}\n",
      "{'word': 'garland', 'score': 0.32437601908284885}\n",
      "{'word': 'thousand', 'score': 0.32315173643451844}\n",
      "{'word': 'isa', 'score': 0.3207973807253339}\n",
      "{'word': 'behave', 'score': 0.3185119561005705}\n",
      "{'word': 'pace', 'score': 0.3179322677448273}\n",
      "{'word': 'essays', 'score': 0.3171284000727035}\n",
      "{'word': 'specified', 'score': 0.31396421868155433}\n",
      "\n",
      "\n",
      "Target:  june\n",
      "{'word': 'june', 'score': 1.0}\n",
      "{'word': 'extraction', 'score': 0.35853753844465264}\n",
      "{'word': 'glendale', 'score': 0.35510756298361457}\n",
      "{'word': 'planning', 'score': 0.35346022772008434}\n",
      "{'word': 'bollywood', 'score': 0.34717614606582625}\n",
      "{'word': 'lok', 'score': 0.3436474831770411}\n",
      "{'word': 'shaft', 'score': 0.33938628103030144}\n",
      "{'word': 'artist', 'score': 0.3227478254357208}\n",
      "{'word': 'merging', 'score': 0.3194166674204194}\n",
      "{'word': 'relates', 'score': 0.3166716371263403}\n",
      "\n",
      "\n",
      "Target:  computer\n",
      "{'word': 'computer', 'score': 1.0}\n",
      "{'word': 'nos', 'score': 0.3824724425937813}\n",
      "{'word': 'threw', 'score': 0.3693474656326228}\n",
      "{'word': 'dts', 'score': 0.3433887456546275}\n",
      "{'word': 'pennsylvania', 'score': 0.3427478281328278}\n",
      "{'word': 'wikimedia', 'score': 0.33531351875326576}\n",
      "{'word': 'settlement', 'score': 0.3306182876478365}\n",
      "{'word': 'arabia', 'score': 0.3190805923743427}\n",
      "{'word': 'cds', 'score': 0.311516964914448}\n",
      "{'word': 'suffolk', 'score': 0.30831516358565625}\n",
      "\n",
      "\n",
      "Target:  european\n",
      "{'word': 'european', 'score': 1.0}\n",
      "{'word': 'dose', 'score': 0.38779526125053576}\n",
      "{'word': 'aspect', 'score': 0.37900056304935226}\n",
      "{'word': 'ton', 'score': 0.35556562588232765}\n",
      "{'word': 'nationalities', 'score': 0.3312667801439868}\n",
      "{'word': 'appliances', 'score': 0.3282534977944215}\n",
      "{'word': 'lisa', 'score': 0.3224007037870176}\n",
      "{'word': 'consciousness', 'score': 0.32078256545223793}\n",
      "{'word': 'beverly', 'score': 0.3207474018133485}\n",
      "{'word': 'bonnet', 'score': 0.3200817331510798}\n",
      "\n",
      "\n",
      "Target:  television\n",
      "{'word': 'television', 'score': 1.0}\n",
      "{'word': 'hand', 'score': 0.370044068576838}\n",
      "{'word': 'hangul', 'score': 0.36729676372416376}\n",
      "{'word': 'consulship', 'score': 0.35313506350732793}\n",
      "{'word': 'breeds', 'score': 0.3454997347324964}\n",
      "{'word': 'bethesda', 'score': 0.3401471682292201}\n",
      "{'word': 'bl', 'score': 0.33801686351663407}\n",
      "{'word': 'hindi', 'score': 0.3346970668126059}\n",
      "{'word': 'practiced', 'score': 0.32993646105551266}\n",
      "{'word': 'logan', 'score': 0.3276323413203106}\n",
      "\n",
      "\n",
      "Target:  meat\n",
      "{'word': 'meat', 'score': 1.0}\n",
      "{'word': 'passport', 'score': 0.3978781386889598}\n",
      "{'word': 'rabbit', 'score': 0.3910075737294618}\n",
      "{'word': 'designation', 'score': 0.37137577003328426}\n",
      "{'word': 'radius', 'score': 0.35415388084876764}\n",
      "{'word': 'weapon', 'score': 0.3513692938448978}\n",
      "{'word': 'surgeon', 'score': 0.3407970461190224}\n",
      "{'word': 'wonderland', 'score': 0.33475213410531257}\n",
      "{'word': 'azerbaijani', 'score': 0.33248909526610415}\n",
      "{'word': 'passed', 'score': 0.3316391702936017}\n",
      "\n",
      "\n",
      "Target:  university\n",
      "{'word': 'university', 'score': 1.0}\n",
      "{'word': 'striking', 'score': 0.3757543321631365}\n",
      "{'word': 'accessible', 'score': 0.35123040316434895}\n",
      "{'word': 'slasher', 'score': 0.3334997837859487}\n",
      "{'word': 'sticky', 'score': 0.328178169341099}\n",
      "{'word': 'tianjin', 'score': 0.3250757321506018}\n",
      "{'word': 'hound', 'score': 0.3078710635880131}\n",
      "{'word': 'mental', 'score': 0.3065582478291198}\n",
      "{'word': 'remainder', 'score': 0.3021878119455521}\n",
      "{'word': 'penalty', 'score': 0.30192709952775587}\n",
      "\n",
      "\n",
      "Target:  mathematics\n",
      "{'word': 'mathematics', 'score': 1.0}\n",
      "{'word': 'nepali', 'score': 0.35048100193721854}\n",
      "{'word': 'pearl', 'score': 0.3483858377094049}\n",
      "{'word': 'meknes', 'score': 0.33469285392073433}\n",
      "{'word': 'sponsored', 'score': 0.3309409762879343}\n",
      "{'word': 'mauritania', 'score': 0.3265565926902847}\n",
      "{'word': 'herbivorous', 'score': 0.32536369825244726}\n",
      "{'word': 'athlete', 'score': 0.3181074798880049}\n",
      "{'word': 'whales', 'score': 0.31734746031551997}\n",
      "{'word': 'factbook', 'score': 0.31476554828827663}\n",
      "\n",
      "\n",
      "Target:  women\n",
      "{'word': 'women', 'score': 1.0}\n",
      "{'word': 'flora', 'score': 0.3936036035788215}\n",
      "{'word': 'snake', 'score': 0.3930323083101487}\n",
      "{'word': 'monte', 'score': 0.38592028785677435}\n",
      "{'word': 'cattle', 'score': 0.3818357327019162}\n",
      "{'word': 'eagles', 'score': 0.3576082652175725}\n",
      "{'word': 'sang', 'score': 0.3573196762331221}\n",
      "{'word': 'farms', 'score': 0.33023574876396644}\n",
      "{'word': 'yuan', 'score': 0.32979448258599375}\n",
      "{'word': 'bandscategory', 'score': 0.3261925839928166}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "filename = 'unlabeled-data.txt'\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "\t#if len(sys.argv)==2:\n",
    "\t\t#filename = sys.argv[1]\n",
    "\t\t#... load in the file, tokenize it and assign each token an index.\n",
    "\t\t#... the full sequence of characters is encoded in terms of their one-hot positions\n",
    "\n",
    "\t\tfullsequence= loadData(filename)\n",
    "\t\tprint (\"Full sequence loaded...\")\n",
    "\t\t#print(uniqueWords)\n",
    "\t\t#print (len(uniqueWords))\n",
    "\n",
    "\n",
    "\n",
    "\t\t#... now generate the negative sampling table\n",
    "\t\tprint (\"Total unique words: \", len(uniqueWords))\n",
    "\t\tprint(\"Preparing negative sampling table\")\n",
    "\t\tsamplingTable = negativeSampleTable(fullsequence, uniqueWords, wordcounts)\n",
    "\n",
    "\n",
    "\t\t#... we've got the word indices and the sampling table. Begin the training.\n",
    "\t\t#... NOTE: If you have already trained a model earlier, preload the results (set preload=True) (This would save you a lot of unnecessary time)\n",
    "\t\t#... If you just want to load an earlier model and NOT perform further training, comment out the train_vectors() line\n",
    "\t\t#... ... and uncomment the load_model() line\n",
    "\n",
    "\t\ttrain_vectors(preload=False)\n",
    "\t\t[word_embeddings, proj_embeddings] = load_model()\n",
    "\t\ttest1 = pd.read_csv('intrinsic-test.tsv' , sep='\\t')\n",
    "        \n",
    "\n",
    "\t\tScorelist = []\n",
    "\t\tfor i in range(0,len(list(test1['word1']))):\n",
    "\t\t\ttry:\n",
    "\t\t\t\t\tword1 = list(test1['word1'])[i]\n",
    "\t\t\t\t\tword2 = list(test1['word2'])[i]\n",
    "\t\t\t\t\tind1 = wordcodes[word1]\n",
    "\t\t\t\t\tvectornumber1 = proj_embeddings[ind1]\n",
    "\t\t\t\t\tind2 = wordcodes[word2]\n",
    "\t\t\t\t\tvectornumber2 = proj_embeddings[ind2]\n",
    "\t\t\t\t\tScore = 1-cosine(vectornumber1,vectornumber2)\n",
    "\t\t\t\t\tScorelist.append(Score)\n",
    "\t\t\texcept:\n",
    "\t\t\t\t\tScorelist.append(0)\n",
    "    \n",
    "\t\ttest1['result'] = Scorelist\n",
    "\t\tfinal = test1.drop(columns = ['word1','word2'])\n",
    "\t\tfinal.columns = ['id', 'sim']\n",
    "\t\tfinal.to_csv('final_new11.csv',index = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t#... we've got the trained weight matrices. Now we can do some predictions\n",
    "\t\ttargets = [ 'coast' ,'london' ,'june',  'computer','european','television','meat','university','mathematics', 'women']\n",
    "\t\tf = open('output1.txt','w')\n",
    "\t\tfor targ in targets:\n",
    "\t\t\tf.writelines(\"\\nTarget: \" + str(targ))\n",
    "\t\t\tprint(\"Target: \", targ)\n",
    "\t\t\tbestpreds= (get_neighbors(targ))\n",
    "\t\t\tfor pred in bestpreds:\n",
    "\t\t\t\tprint (pred)\n",
    "\t\t\t\tf.writelines(\"\\n\" + str(pred))\n",
    "\t\t\tprint (\"\\n\")\n",
    "\t\tf.close()\n",
    "\n",
    "\n",
    "\t\t#... try morphological task. Input is averages of vector combinations that use some morphological change.\n",
    "\t\t#... see how well it predicts the expected target word when using word_embeddings vs proj_embeddings in\n",
    "\t\t#... the morphology() function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\tsys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_new1.csv')\n",
    "\n",
    "df.columns = ['id', 'sim']\n",
    "\n",
    "df.to_csv('final_new11.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 - Q5 finished\n",
    "# Q6 : Write a functionget neighbors(word)so that it takes one argument, the tar-get word, and computes the top 10 most\n",
    "#similar words based on the cosine similarity.\n",
    "\n",
    "#Problem 7.Pick 10 target words and compute the most similar for each using your function.Record  these  \n",
    "#in  a  file  namedprob7output.txtQualitatively  looking  at  the  most  similarwords for each target word, \n",
    "#do these predicted word seem to be semantically similar to the targetword? \n",
    "#Describe what you see in 2-3 sentences.Hint:For maximum effect, \n",
    "#try picking wordsacross a range of frequencies (common, occasional, rare words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual\n",
    "# subject\n",
    " #subjectivitly Our multiple path, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
